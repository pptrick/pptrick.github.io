{"pageProps":{"pubData":[{"id":"DTTDv1","img_path":"/resource/publication/DTTDv1/dttdv1.png","title":"Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications","author":"Weiyu Feng*, Seth Z. Zhao*, Chuanyu Pan*, Adam Chang, Yichen Chen, Zekun Wang, Allen Y. Yang","date":"2023-02-12","paper":"https://arxiv.org/abs/2302.05991","projectPage":"https://github.com/augcog/DTTDv1","content":"Digital twin is a problem of augmenting real objects with their digital counterparts. It can underpin a wide range of applications in augmented reality (AR), autonomy, and UI/UX. A critical component in a good digital twin system is real-time, accurate 3D object tracking. Most existing works solve 3D object tracking through the lens of robotic grasping, employ older generations of depth sensors, and measure performance metrics that may not apply to other digital twin applications such as in AR. In this work, we create a novel RGB-D dataset, called Digital-Twin Tracking Dataset (DTTD), to enable further research of the problem and extend potential solutions towards longer ranges and mm localization accuracy. To reduce point cloud noise from the input source, we select the latest Microsoft Azure Kinect as the state-of-the-art time-of-flight (ToF) camera. In total, 103 scenes of 10 common off-the-shelf objects with rich textures are recorded, with each frame annotated with a per-pixel semantic segmentation and ground-truth object poses provided by a commercial motion capturing system. Through experiments, we demonstrate that DTTD can help researchers develop future object tracking methods and analyze new challenges. "},{"id":"ObjectPursuit","img_path":"/resource/publication/ObjectPursuit/img.jpg","title":"Object Pursuit: Building a Space of Objects via Discriminative Weight Generation","author":"Chuanyu Pan*, Yanchao Yang*, Kaichun Mo, Yueqi Duan, Leonidas J. Guibas","conference":"The International Conference on Learning Representations (ICLR), 2022","date":"2022-01-24","paper":"https://arxiv.org/pdf/2112.07954.pdf","projectPage":"https://pptrick.github.io/static/object-pursuit/index.html","video":"https://iclr.cc/virtual/2022/poster/6713","content":"We propose a framework to continuously learn object-centric representations for visual learning and understanding. Our method leverages interactions to effectively sample diverse variations of an object and the corresponding training signals while learning the object-centric representations. Throughout learning, objects are streamed one by one in random order with unknown identities, and are associated with latent codes that can synthesize discriminative weights for each object through a convolutional hypernetwork."},{"id":"Robust3DPortraits","img_path":"/resource/publication/Robust3DPortraits/img.jpg","title":"Robust 3D Self-portraits in Seconds","author":"Zhe Li, Tao Yu, Chuanyu Pan, Zerong Zheng, Yebin Liu","conference":"IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020 (Oral presentation)","date":"2020-03-15","projectPage":"http://www.liuyebin.com/portrait/portrait.html","paper":"https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Robust_3D_Self-Portraits_in_Seconds_CVPR_2020_paper.pdf","video":"http://www.liuyebin.com/portrait/assets/portrait.mp4","content":"We propose an efficient method for robust 3D self-portraits using a single RGBD camera. Benefiting from the proposed PIFusion and lightweight bundle adjustment algorithm, our method can generate detailed 3D self-portraits in seconds and shows the ability to handle extremely loose clothes. To achieve highly efficient and robust reconstruction, we contribute PIFusion, which combines learning-based 3D recovery with volumetric non-rigid fusion to generate accurate sparse partial scans of the performer."}]},"__N_SSG":true}