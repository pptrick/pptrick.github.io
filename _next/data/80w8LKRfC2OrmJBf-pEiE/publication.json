{"pageProps":{"pubData":[{"id":"ObjectPursuit","img_path":"\\resource\\publication\\ObjectPursuit\\img.jpg","title":"Object Pursuit: Building a Space of Objects via Discriminative Weight Generation","author":"Chuanyu Pan*, Yanchao Yang*, Kaichun Mo, Yueqi Duan, Leonidas J. Guibas","conference":"The International Conference on Learning Representations (ICLR), 2022","date":"2022-01-24","content":"We propose a framework to continuously learn object-centric representations for visual learning and understanding. Our method leverages interactions to effectively sample diverse variations of an object and the corresponding training signals while learning the object-centric representations. Throughout learning, objects are streamed one by one in random order with unknown identities, and are associated with latent codes that can synthesize discriminative weights for each object through a convolutional hypernetwork."},{"id":"Robust3DPortraits","img_path":"\\resource\\publication\\Robust3DPortraits\\img.jpg","title":"Robust 3D Self-portraits in Seconds","author":"Zhe Li, Tao Yu, Chuanyu Pan, Zerong Zheng, Yebin Liu","conference":"IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020 (Oral presentation)","date":"2020-03-15","content":"In this paper, we propose an efficient method for robust 3D self-portraits using a single RGBD camera. Benefiting from the proposed PIFusion and lightweight bundle adjustment algorithm, our method can generate detailed 3D self-portraits in seconds and shows the ability to handle extremely loose clothes. To achieve highly efficient and robust reconstruction, we contribute PIFusion, which combines learning-based 3D recovery with volumetric non-rigid fusion to generate accurate sparse partial scans of the performer. Moreover, a non-rigid volumetric deformation method is proposed to continuously refine the learned shape prior. Finally, a lightweight bundle adjustment algorithm is proposed to guarantee that all the partial scans can not only ``loop'' with each other, but also keep consistent with the selected live key observations. Results and experiments show that the proposed method achieves more robust and efficient 3D self-portraits compared with state-of-the-art methods."}]},"__N_SSG":true}